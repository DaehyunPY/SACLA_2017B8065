{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from itertools import chain, product\n",
    "from functools import reduce, partial\n",
    "from glob import iglob\n",
    "\n",
    "from yaml import safe_load\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as f\n",
    "from dltools import load_combiner\n",
    "from dltools.sacla import restructure, load_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config file...\n",
      "Loading momentum model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load config file\n",
    "with open(\"Global; Config.yaml\", \"r\") as file:\n",
    "    print(\"Loading config file...\")\n",
    "    config = safe_load(file)\n",
    "\n",
    "# %% Load momentum model\n",
    "print(\"Loading momentum model...\")\n",
    "analyzer = load_analyzer(config[\"momentum_analyzer\"].copy())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PySpark...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load PySpark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.0,\"\n",
    "        \"org.diana-hep:spark-root_2.11:0.1.15,\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Loading PySpark...\")\n",
    "spark = builder.getOrCreate()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "root\n",
      " |-- tag: long (nullable = true)\n",
      " |-- hits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- t: double (nullable = false)\n",
      " |    |    |-- x: double (nullable = false)\n",
      " |    |    |-- y: double (nullable = false)\n",
      " |    |    |-- as_: map (nullable = false)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |    |-- pz: double (nullable = false)\n",
      " |    |    |    |    |-- px: double (nullable = false)\n",
      " |    |    |    |    |-- py: double (nullable = false)\n",
      " |    |    |    |    |-- ke: double (nullable = false)\n",
      " |    |    |-- flag: integer (nullable = true)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load data\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, LongType, StructField, StructType, DoubleType,\n",
    ")\n",
    "from dltools import SpkHits\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"uri\", \"mongodb://mongodb/sacla_2017b8065.resorted\")\n",
    "    .option(\"pipeline\", \"\"\"[\n",
    "        {\n",
    "            $match: {\n",
    "                aq: {$in: [28, 29]},\n",
    "            },\n",
    "        },\n",
    "    ]\"\"\")\n",
    "    .schema(\n",
    "        StructType([\n",
    "            StructField(\"tag\", LongType()),\n",
    "            StructField(\"hits\", SpkHits)\n",
    "        ])\n",
    "    )\n",
    "    .load()\n",
    "#     .cache()\n",
    ")\n",
    "df.printSchema()\n",
    "# df.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the data...\n",
      "root\n",
      " |-- tag: long (nullable = true)\n",
      " |-- hits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- t: double (nullable = false)\n",
      " |    |    |-- x: double (nullable = false)\n",
      " |    |    |-- y: double (nullable = false)\n",
      " |    |    |-- as_: map (nullable = false)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |    |-- pz: double (nullable = false)\n",
      " |    |    |    |    |-- px: double (nullable = false)\n",
      " |    |    |    |    |-- py: double (nullable = false)\n",
      " |    |    |    |    |-- ke: double (nullable = false)\n",
      " |    |    |-- flag: integer (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+-------------------+------------------+\n",
      "|as_|                 pz|                 px|                 py|                ke|\n",
      "+---+-------------------+-------------------+-------------------+------------------+\n",
      "|H1p| 21.710403206690547|-7.5381700927181585|   17.8690418025858| 6.275514499538849|\n",
      "|H1p| 14.848950576109704|  20.95545452495596|  -87.7110130588647| 61.85292927675093|\n",
      "|C2p|-323.04121277250044|-101.24545812955249| -31.07485233617065| 71.88388497638157|\n",
      "|C2p| -263.9257184639096| -211.3842132364199|   6.76849842543329| 71.14619242107803|\n",
      "|C2p|-240.30996326713154| 246.88874170650124| -18.59884794179165| 74.04648114019615|\n",
      "|I5p| 1115.4196391521086|  658.8471867313713| 1551.0003954991905| 240.1972116219671|\n",
      "|I4p|  -300.861785060529| 102.23995631663081|-48.880779463147974| 6.079279166887332|\n",
      "|I4p|-169.67829712314656| -757.2528327625637| 277.92299331689946| 39.96365917737601|\n",
      "|I4p| -99.15287189174254|-274.33062874616667| -163.0080658217331|  6.56745952540033|\n",
      "|I4p|-127.04830788480021| -918.9477239658898| -507.6331695423441|  65.7743169752447|\n",
      "|I3p| 506.13428348805934|-1826.4520319339238|  543.7167865700351| 228.6623651232639|\n",
      "|H1p| -76.89648864263108| 21.665792956903246|  -99.8918380044017|121.15236836873079|\n",
      "|H1p|-10.335953198156297| 15.889124516137144|-5.9671826234577106|2.9242658022222288|\n",
      "|C2p|-387.26906807077785|  6.927273102142356| -41.60146688343086| 94.38980848997768|\n",
      "|C1p| -7.453073616073731|  43.08791841460068|-137.06827332004718|12.874962778481645|\n",
      "|I5p|  -808.512141073099|  76.22019066785802|  512.0650375502416| 54.21184482273876|\n",
      "|I5p|  -849.805651665801|  987.9094979421093| 397.88674457275465|109.18975523133408|\n",
      "|I5p| -760.3783469950014| -659.8110203939307| -1686.027255455138|226.80887219677507|\n",
      "|I5p| 1362.8173647071696| 138.30532364805168|-1308.7893237977287| 211.1116570865676|\n",
      "|I4p|-1090.3407630859597| 255.51354694972972| 1076.2015011871968|141.88530889075537|\n",
      "+---+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Analyze the data\n",
    "print(\"Analyzing the data...\")\n",
    "analyzed = (\n",
    "    df\n",
    "    .withColumn(\"hits\", analyzer(f.col(\"hits\")))\n",
    ")\n",
    "analyzed.printSchema()\n",
    "\n",
    "(\n",
    "    analyzed\n",
    "    .select(f.explode(\"hits\").alias(\"h\"))\n",
    "    .select(f.explode(\"h.as_\").alias(\"as_\", \"h\"))\n",
    "    .select(\n",
    "        \"as_\",\n",
    "        f.col(\"h.pz\").alias(\"pz\"),\n",
    "        f.col(\"h.px\").alias(\"px\"),\n",
    "        f.col(\"h.py\").alias(\"py\"),\n",
    "        f.col(\"h.ke\").alias(\"ke\"),\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Insert data to MongoDB\n",
    "print(\"Updating data...\")\n",
    "(\n",
    "    analyzed\n",
    "    .write\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"uri\", \"mongodb://mongodb/sacla_2017b8065.resorted\")\n",
    "    .option(\"replaceDocument\", \"false\")\n",
    "    .option(\"shardKey\", \"{tag: true}\")\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
