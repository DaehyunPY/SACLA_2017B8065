{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from itertools import chain, product\n",
    "from functools import reduce, partial\n",
    "from glob import iglob\n",
    "\n",
    "from yaml import safe_load\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as f\n",
    "from dltools import load_combiner\n",
    "from dltools.sacla import restructure, load_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config file...\n",
      "Loading momentum model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load config file\n",
    "with open(\"aq035--aq036; Config.yaml\", \"r\") as file:\n",
    "    print(\"Loading config file...\")\n",
    "    config = safe_load(file)\n",
    "\n",
    "# %% Load momentum model\n",
    "print(\"Loading momentum model...\")\n",
    "analyzer = load_analyzer(config[\"momentum_analyzer\"].copy())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PySpark...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load PySpark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:2.7.0,\"\n",
    "        \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.1,\"\n",
    "        \"org.diana-hep:spark-root_2.11:0.1.15,\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Loading PySpark...\")\n",
    "spark = builder.getOrCreate()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "root\n",
      " |-- tag: long (nullable = true)\n",
      " |-- hits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- t: double (nullable = false)\n",
      " |    |    |-- x: double (nullable = false)\n",
      " |    |    |-- y: double (nullable = false)\n",
      " |    |    |-- as_: map (nullable = false)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |    |-- pz: double (nullable = false)\n",
      " |    |    |    |    |-- px: double (nullable = false)\n",
      " |    |    |    |    |-- py: double (nullable = false)\n",
      " |    |    |    |    |-- ke: double (nullable = false)\n",
      " |    |    |-- flag: integer (nullable = true)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load data\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, BooleanType, LongType, StructField, StructType, DoubleType,\n",
    ")\n",
    "from dltools import SpkHits\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"uri\", \"mongodb://mongodb/sacla_2017b8065.resorted\")\n",
    "    .option(\"pipeline\", \"\"\"[\n",
    "        {\n",
    "            $match: {\n",
    "                aq: {$in: [35, 36]},\n",
    "            },\n",
    "        },\n",
    "    ]\"\"\")\n",
    "    .schema(\n",
    "        StructType([\n",
    "            StructField(\"tag\", LongType()),\n",
    "            StructField(\"hits\", SpkHits)\n",
    "        ])\n",
    "    )\n",
    "    .load()\n",
    "#     .cache()\n",
    ")\n",
    "df.printSchema()\n",
    "# df.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the data...\n",
      "root\n",
      " |-- tag: long (nullable = true)\n",
      " |-- hits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- t: double (nullable = false)\n",
      " |    |    |-- x: double (nullable = false)\n",
      " |    |    |-- y: double (nullable = false)\n",
      " |    |    |-- as_: map (nullable = false)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |    |-- pz: double (nullable = false)\n",
      " |    |    |    |    |-- px: double (nullable = false)\n",
      " |    |    |    |    |-- py: double (nullable = false)\n",
      " |    |    |    |    |-- ke: double (nullable = false)\n",
      " |    |    |-- flag: integer (nullable = true)\n",
      "\n",
      "+---+-------------------+-------------------+-------------------+------------------+\n",
      "|as_|                 pz|                 px|                 py|                ke|\n",
      "+---+-------------------+-------------------+-------------------+------------------+\n",
      "|H1p| -94.64085870354376| -37.85135724690025| -22.51806040189454| 80.69007227116026|\n",
      "|C2p| -349.6958812976832|  5.742241753179413| -5.049838954923045| 76.09706124963853|\n",
      "|C1p|  2.243271252876492| -59.66833858613199|-108.58262244936338| 9.550893669796853|\n",
      "|H1p| -5.709998018039238|-15.374375876466436|-2.6897722755648883|2.0453418036866977|\n",
      "|C2p|-193.36578061001188|  443.2679470913329| -15.74512560321817|145.62182860582607|\n",
      "|H1p| -119.7255956442039|-28.743313327473455|-53.691653119799525|133.61007595039808|\n",
      "|C2p|-352.12072305727764|  7.007129296745632| -59.53446519783895| 79.35425004919244|\n",
      "|C1p| -22.29411277027441|  -68.9327252299305|-17.557501360040607| 3.456373912028622|\n",
      "|I4p| -201.3425746012869|  485.1680273253794| 328.68169450013323|22.583083164249295|\n",
      "|I4p|-164.59628362193925|-509.26924747643915|-30.775799643228744|16.903514978411035|\n",
      "|I4p|-105.23997103079282| -415.5088953385858| -83.71474544565575|11.218138660225351|\n",
      "|I2p| -310.3209900582414|-22.214031946448497|  9.039672756414536| 5.697803951608336|\n",
      "|I2p|  440.6361910901574|  26.96506065542808| 11.553149574797725|11.470437220986838|\n",
      "|H1p|  -95.8792254624486| -79.39459550437226| 17.396183049844176|116.99148357684594|\n",
      "|H1p|-49.168867971918786|  34.44564662967061| -7.159049466945369|27.067786051890916|\n",
      "|H1p| 1.8983806201700248|  24.62390465921912|  44.48544938088716|19.170809448199854|\n",
      "|H1p| 15.390987431798605| -5.925493427931495|-0.4998880344796499|2.0159695833510898|\n",
      "|I4p| -1251.084651098927|-128.73506599286318| -70.51172325882177| 93.32746784905385|\n",
      "|I4p|   803.274469172062| 125.94217783084105|  78.15483981213333| 39.24349366499169|\n",
      "|H1p|  6.485371181648333| -9.852262940354908| 13.220495310306028|2.3244977642445894|\n",
      "+---+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Analyze the data\n",
    "print(\"Analyzing the data...\")\n",
    "analyzed = (\n",
    "    df\n",
    "    .withColumn(\"hits\", analyzer(f.col(\"hits\")))\n",
    ")\n",
    "analyzed.printSchema()\n",
    "\n",
    "(\n",
    "    analyzed\n",
    "    .select(f.explode(\"hits\").alias(\"h\"))\n",
    "    .select(f.explode(\"h.as_\").alias(\"as_\", \"h\"))\n",
    "    .select(\n",
    "        \"as_\",\n",
    "        f.col(\"h.pz\").alias(\"pz\"),\n",
    "        f.col(\"h.px\").alias(\"px\"),\n",
    "        f.col(\"h.py\").alias(\"py\"),\n",
    "        f.col(\"h.ke\").alias(\"ke\"),\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Insert data to MongoDB\n",
    "print(\"Updating data...\")\n",
    "(\n",
    "    analyzed\n",
    "    .write\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"uri\", \"mongodb://mongodb/sacla_2017b8065.resorted\")\n",
    "    .option(\"replaceDocument\", \"false\")\n",
    "    .option(\"shardKey\", \"{tag: true}\")\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
