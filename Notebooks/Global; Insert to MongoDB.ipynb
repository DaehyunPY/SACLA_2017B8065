{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from itertools import chain, compress\n",
    "from functools import reduce, partial\n",
    "from glob import iglob\n",
    "from typing import Generator\n",
    "\n",
    "from numba import jit\n",
    "from yaml import safe_load\n",
    "from cytoolz import compose, merge_with\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession, Row, DataFrame, functions as f\n",
    "from pyspark.sql.types import ArrayType, BooleanType\n",
    "from dltools import SpkHits, load_combiner\n",
    "from dltools.sacla import restructure, load_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PySpark...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load PySpark\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:2.7.0,\"\n",
    "        \"org.mongodb.spark:mongo-spark-connector_2.11:2.3.1,\"\n",
    "        \"org.diana-hep:spark-root_2.11:0.1.15,\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Loading PySpark...\")\n",
    "spark = builder.getOrCreate()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "root\n",
      " |-- tag: long (nullable = true)\n",
      " |-- hits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- t: double (nullable = false)\n",
      " |    |    |-- x: double (nullable = false)\n",
      " |    |    |-- y: double (nullable = false)\n",
      " |    |    |-- as_: map (nullable = false)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |    |    |-- pz: double (nullable = false)\n",
      " |    |    |    |    |-- px: double (nullable = false)\n",
      " |    |    |    |    |-- py: double (nullable = false)\n",
      " |    |    |    |    |-- ke: double (nullable = false)\n",
      " |    |    |-- flag: integer (nullable = true)\n",
      "\n",
      "+---------+--------------------+\n",
      "|      tag|                hits|\n",
      "+---------+--------------------+\n",
      "|158648231|[[803.53128890943...|\n",
      "|158648232|[[804.66343712302...|\n",
      "|158648233|[[794.79463683844...|\n",
      "|158648234|[[786.43060318885...|\n",
      "|158648235|[[709.28913731595...|\n",
      "|158648236|[[627.36191628571...|\n",
      "|158648237|[[899.62290167172...|\n",
      "|158648238|[[739.34554237618...|\n",
      "|158648239|[[717.03839884343...|\n",
      "|158648240|[[763.52143977673...|\n",
      "|158648241|[[788.73078322175...|\n",
      "|158648242|[[782.42968366127...|\n",
      "|158648243|[[625.77740580072...|\n",
      "|158648244|[[723.58140869737...|\n",
      "|158648245|[[673.65119094314...|\n",
      "|158648246|[[876.13232076132...|\n",
      "|158648247|[[845.97914411088...|\n",
      "|158648248|[[731.92747809820...|\n",
      "|158648249|[[762.29131957490...|\n",
      "|158648250|[[733.65021891392...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %% Load data\n",
    "files = [\n",
    "    \"/helium/analysis/SACLA_2017B8065_Takanashi/resort_201809/aq028--aq029/aq028_aq029_SortEvent_aq.root\",\n",
    "    \"/helium/analysis/SACLA_2017B8065_Takanashi/resort_201809/aq030/aq030_SortEvent_aq.root\",\n",
    "    \"/helium/analysis/SACLA_2017B8065_Takanashi/resort_201809/aq032/aq032_SortEvent_aq.root\",\n",
    "    \"/helium/analysis/SACLA_2017B8065_Takanashi/resort_201809/aq033--aq034/aq033_aq034_SortEvent_aq.root\"\n",
    "    \"/helium/analysis/SACLA_2017B8065_Takanashi/resort_201809/aq035--aq036/aq035_aq036_SortEvent_aq.root\",\n",
    "]\n",
    "\n",
    "print(\"Loading data...\")\n",
    "loaded = (spark.read.format(\"org.dianahep.sparkroot\").load(f) for f in files)\n",
    "df = restructure(reduce(DataFrame.union, loaded))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Insert data to MongoDB\n",
    "print(\"Inserting data...\")\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"uri\", \"mongodb://mongodb/sacla_2017b8065.resorted\")\n",
    "    .option(\"replaceDocument\", \"false\")\n",
    "    .option(\"shardKey\", \"{tag: true}\")\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
